{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":94327,"sourceType":"datasetVersion","datasetId":50445},{"sourceId":3103519,"sourceType":"datasetVersion","datasetId":1895328},{"sourceId":13681197,"sourceType":"datasetVersion","datasetId":8650979},{"sourceId":275302754,"sourceType":"kernelVersion"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install loguru\n!pip install seqeval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:36:56.967314Z","iopub.execute_input":"2025-12-17T15:36:56.968410Z","iopub.status.idle":"2025-12-17T15:37:04.159150Z","shell.execute_reply.started":"2025-12-17T15:36:56.968377Z","shell.execute_reply":"2025-12-17T15:37:04.158088Z"},"_kg_hide-output":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: loguru in /usr/local/lib/python3.11/dist-packages (0.7.3)\nRequirement already satisfied: seqeval in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.26.4)\nRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.2.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.14.0->seqeval) (2.4.1)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.15.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.14.0->seqeval) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.14.0->seqeval) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.14.0->seqeval) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.14.0->seqeval) (2024.2.0)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport json\nimport tqdm\nfrom loguru import logger\nfrom transformers import BertTokenizerFast, BertModel, AutoModel\nfrom torch.optim import AdamW\nimport torch.utils.data as Data\nfrom torch.utils.data import DataLoader, Dataset\nfrom seqeval.metrics import f1_score, classification_report, accuracy_score\n%config Completer.use_jedi = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:37:04.161175Z","iopub.execute_input":"2025-12-17T15:37:04.161480Z","iopub.status.idle":"2025-12-17T15:37:04.170191Z","shell.execute_reply.started":"2025-12-17T15:37:04.161452Z","shell.execute_reply":"2025-12-17T15:37:04.169379Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"#读取数据并做基本处理\nsentences = []\ncur_token = []\ncur_ner = []\nwith open('/kaggle/input/conll003-englishversion/train.txt','r') as f:    \n    for line in f:\n        line = line.strip()\n        if not line: #空行 即句子结束\n            if cur_token:\n                #sentences是一个包含{token ner_tag}字典的列表\n                sentences.append({\n                    'token':cur_token,\n                    'ner_tag':cur_ner})\n                cur_token = []\n                cur_ner = []\n            continue\n        line = line.split()\n        if len(line) >= 4:\n            token = line[0]\n            ner_tag = line[3]\n\n            cur_token.append(token)\n            cur_ner.append(ner_tag)\n            ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:37:04.171257Z","iopub.execute_input":"2025-12-17T15:37:04.171774Z","iopub.status.idle":"2025-12-17T15:37:04.845882Z","shell.execute_reply.started":"2025-12-17T15:37:04.171742Z","shell.execute_reply":"2025-12-17T15:37:04.845179Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"#创建label index映射\ndef create_label_mapping(sentences):\n    label = set()\n    for i in sentences:\n        label.update(i['ner_tag'])\n\n    label2idx = {k: v for v, k in enumerate(sorted(label))}\n    idx2label = {v: k for v, k in enumerate(sorted(label))}\n    \n        \n    return label2idx, idx2label\n    \nlabel2idx, idx2label = create_label_mapping(sentences)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:37:04.846680Z","iopub.execute_input":"2025-12-17T15:37:04.846971Z","iopub.status.idle":"2025-12-17T15:37:04.859734Z","shell.execute_reply.started":"2025-12-17T15:37:04.846946Z","shell.execute_reply":"2025-12-17T15:37:04.858873Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"print(label2idx)\nprint(idx2label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:37:04.861965Z","iopub.execute_input":"2025-12-17T15:37:04.862714Z","iopub.status.idle":"2025-12-17T15:37:04.878552Z","shell.execute_reply.started":"2025-12-17T15:37:04.862685Z","shell.execute_reply":"2025-12-17T15:37:04.877622Z"}},"outputs":[{"name":"stdout","text":"{'B-LOC': 0, 'B-MISC': 1, 'B-ORG': 2, 'B-PER': 3, 'I-LOC': 4, 'I-MISC': 5, 'I-ORG': 6, 'I-PER': 7, 'O': 8}\n{0: 'B-LOC', 1: 'B-MISC', 2: 'B-ORG', 3: 'B-PER', 4: 'I-LOC', 5: 'I-MISC', 6: 'I-ORG', 7: 'I-PER', 8: 'O'}\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"tokenizer = BertTokenizerFast.from_pretrained('/kaggle/input/bert-base-cased')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:37:04.879586Z","iopub.execute_input":"2025-12-17T15:37:04.879974Z","iopub.status.idle":"2025-12-17T15:37:04.994461Z","shell.execute_reply.started":"2025-12-17T15:37:04.879948Z","shell.execute_reply":"2025-12-17T15:37:04.993784Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# 构建数据集，此处采用动态构建的方式，即在模型训练时对每一条样本进行处理\nclass NERDataset(Dataset):\n    def __init__(self, sentences, tokenizer,label2idx, max_length = 128):\n        self.sentences = sentences\n        self.tokenizer = tokenizer\n        self.label2idx = label2idx\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.sentences)\n\n    def __getitem__(self, idx): # 此处是动态地处理数据，在训练中处理\n        # logger.info(f'now processing the {idx}th sentence')\n        tokens, labels = self.sentences[idx]['token'], self.sentences[idx]['ner_tag']\n\n        encoding = self.tokenizer( #等价于 tokenizer.encode_plus()\n            tokens,\n            is_split_into_words = True, #已切分\n            padding = 'max_length',\n            max_length = self.max_length,\n            truncation = True,\n            return_tensors = 'pt'\n        )\n\n        #标签对齐\n        word_ids = encoding.word_ids() #获取token在原始序列中的序号\n        aligned_labels = []\n        previous_word_idx = None\n\n        for word_idx in word_ids:\n            if word_idx is None: \n                aligned_labels.append(-100)\n            elif word_idx != previous_word_idx:\n                # 新单词的第一个subword，使用原始标签\n                aligned_labels.append(label2idx[labels[word_idx]])\n            else:\n                # 原单词的后续subword，需要被忽略\n                #这也是BERT的sub-word level与NER作为word level问题之间的gap\n                aligned_labels.append(-100)\n            previous_word_idx = word_idx\n        return {\n            'input_ids':encoding['input_ids'].squeeze(0),#去掉batch维度\n            'attention_mask':encoding['attention_mask'].squeeze(0),\n            'labels': torch.tensor(aligned_labels, dtype=torch.long),\n            'word_ids':word_ids\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:37:04.995247Z","iopub.execute_input":"2025-12-17T15:37:04.995452Z","iopub.status.idle":"2025-12-17T15:37:05.003401Z","shell.execute_reply.started":"2025-12-17T15:37:04.995436Z","shell.execute_reply":"2025-12-17T15:37:05.002528Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# 划分训练集和测试集\nfrom sklearn.model_selection import train_test_split\ntrain_sentences, val_sentences = train_test_split(\n    sentences, test_size=0.2, random_state=42, shuffle=True\n)\ntrain_set = NERDataset(train_sentences, tokenizer,label2idx)\nval_set = NERDataset(val_sentences, tokenizer,label2idx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:37:05.004229Z","iopub.execute_input":"2025-12-17T15:37:05.004512Z","iopub.status.idle":"2025-12-17T15:37:05.041849Z","shell.execute_reply.started":"2025-12-17T15:37:05.004489Z","shell.execute_reply":"2025-12-17T15:37:05.040959Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# 创建DataLoader\n# collate_fn的作用就是按照需要的方式堆叠张量\ndef ner_collate_fn(data):\n    input_ids = torch.stack([item['input_ids'] for item in data])\n    attn_mask = torch.stack([item['attention_mask'] for item in data])\n    labels = torch.stack([item['labels'] for item in data])\n    word_ids = [item.get('word_ids', []) for item in data] \n    return {'input_ids':input_ids, 'attention_mask':attn_mask, 'labels':labels, 'word_ids':word_ids}\n    \ntrainloader = DataLoader(\n    train_set, \n    batch_size=100,\n    shuffle = True,\n    collate_fn = ner_collate_fn\n)\nvalloader = DataLoader(\n    val_set, \n    batch_size=100,\n    shuffle = True,\n    collate_fn = ner_collate_fn\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:37:05.042754Z","iopub.execute_input":"2025-12-17T15:37:05.042982Z","iopub.status.idle":"2025-12-17T15:37:05.048944Z","shell.execute_reply.started":"2025-12-17T15:37:05.042965Z","shell.execute_reply":"2025-12-17T15:37:05.048140Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# 构建模型\nclass Bert(nn.Module):\n    def __init__(self,num_labels):\n        super(Bert, self).__init__()\n        self.bert = AutoModel.from_pretrained('/kaggle/input/bert-base-cased')\n        for param in self.bert.parameters(): #此处是全部参数参与更新\n            param.require_grads = True\n        #NER是对每个位置的token进行分类\n        self.classifier = nn.Linear(768, num_labels)\n\n        #可选: 加上Droupout\n        self.dropout = nn.Dropout(0.2)\n\n    def forward(self, input_ids, attention_mask):\n        output = self.bert(\n            input_ids = input_ids,\n            attention_mask = attention_mask,\n            return_dict = True\n        )\n        # 对于NER等序列标注最后一层的隐藏层输出last_hidden_state, size = [batch_size,seq_len,hidden_dim]\n        seq_output = output.last_hidden_state\n        if self.training == True:\n            seq_output = self.dropout(seq_output)\n        # size = [batch_size, seq_len, num_labels]\n        logits = self.classifier(seq_output)\n\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:37:05.049879Z","iopub.execute_input":"2025-12-17T15:37:05.050997Z","iopub.status.idle":"2025-12-17T15:37:05.064216Z","shell.execute_reply.started":"2025-12-17T15:37:05.050976Z","shell.execute_reply":"2025-12-17T15:37:05.063554Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# 定义模型、优化器和损失函数\nnum_labels = len(label2idx)\ndevice ='cuda' if torch.cuda.is_available() else 'cpu'\n\nmodel = Bert(num_labels).to(device)\n\noptimizer =AdamW(model.parameters(), lr=2e-5)\nloss_func = nn.CrossEntropyLoss(ignore_index=-100)# 对于-100不参与损失计算，即[CLS] [SEP]等\n\nnum_epoch = 3","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:37:05.064924Z","iopub.execute_input":"2025-12-17T15:37:05.065179Z","iopub.status.idle":"2025-12-17T15:37:15.126958Z","shell.execute_reply.started":"2025-12-17T15:37:05.065152Z","shell.execute_reply":"2025-12-17T15:37:15.126239Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# 训练流程\n# 训练阶段是subword级别的\nmodel.train()\nfor i in range(num_epoch):\n    acc_loss = 0\n    for idx, batch in enumerate(trainloader):\n        input_ids = batch['input_ids'].to(device)\n        attention_masks = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        \n        optimizer.zero_grad()\n    \n        output = model(input_ids, attention_masks)\n\n        loss = loss_func(\n            output.view(-1, output.size(-1)),\n            labels.view(-1)\n        )\n        \n        loss.backward()\n        optimizer.step()\n        acc_loss += loss.item()\n        \n        if idx %100 == 0:\n            avg_loss = acc_loss / (idx + 1) if idx > 0 else acc_loss\n            print(f'epoch{i+1} batch{idx + 1}的 acc_loss：{avg_loss:.4f}')\n\n    epoch_avg_loss = acc_loss / len(trainloader)\n    print(f'Epoch {i+1} finished. Average Loss: {epoch_avg_loss:.4f}')\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:37:15.127887Z","iopub.execute_input":"2025-12-17T15:37:15.128145Z","iopub.status.idle":"2025-12-17T15:49:16.036220Z","shell.execute_reply.started":"2025-12-17T15:37:15.128125Z","shell.execute_reply":"2025-12-17T15:49:16.035397Z"},"_kg_hide-input":true,"_kg_hide-output":true},"outputs":[{"name":"stdout","text":"epoch1 batch1的 acc_loss：2.4830\nepoch1 batch101的 acc_loss：0.4677\nEpoch 1 finished. Average Loss: 0.4193\nepoch2 batch1的 acc_loss：0.1168\nepoch2 batch101的 acc_loss：0.1059\nEpoch 2 finished. Average Loss: 0.1029\nepoch3 batch1的 acc_loss：0.0476\nepoch3 batch101的 acc_loss：0.0597\nEpoch 3 finished. Average Loss: 0.0593\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"def align_pred2entity(preds, labels, word_ids, idx2label):\n    \"\"\"\n    将subword级别的预测转换回word级别\n    preds和labels都是subword级别的idx\n    需要遍历每个word_idx, 若等于previous_word_idx则说明是同一个word, 可跳过\n    \"\"\"\n    true_labels = []\n    pred_labels = []\n\n    previous_word_idx = None\n    for i, (pred, label, word_idx) in enumerate(zip(preds, labels, word_ids)):\n        if word_idx is None: #特殊字符 [CLS]等\n            continue\n        if word_idx != previous_word_idx:\n            if label != -100:\n                true_labels.append(idx2label[label])\n                pred_labels.append(idx2label[pred])\n            else:\n                #对于每个词的第一个subword就是-100的情况，实则是数据问题，用O来处理\n                true_labels.append('O')\n                pred_labels.append(idx2label[pred])\n        previous_word_idx = word_idx\n\n    return true_labels, pred_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:49:16.037179Z","iopub.execute_input":"2025-12-17T15:49:16.037413Z","iopub.status.idle":"2025-12-17T15:49:16.043087Z","shell.execute_reply.started":"2025-12-17T15:49:16.037395Z","shell.execute_reply":"2025-12-17T15:49:16.042291Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"#模型评估\n#注意模型评估阶段，是要汇总到实体(span) level的，这一点与训练阶段不同\n\nmodel.eval()\nmodel.to(device)\n\ntrue_list = []\npred_list = []\nwith torch.no_grad():\n    for idx, batch in enumerate(valloader):\n        input_ids = batch['input_ids'].to(device)\n        attention_masks = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        word_ids = batch['word_ids']\n    \n        output = model(input_ids, attention_masks)\n        prediction = torch.argmax(output, dim=2) #沿着num_labels\n    \n        # 处理这个batch中的每个样本\n        batch_size = batch['input_ids'].shape[0]\n        for i in range(batch_size):\n            seq_len = attention_masks[i].sum().item()\n            # 获取当前样本的word_ids，并且截取有效长度\n            w_ids = word_ids[i][:seq_len] \n            true_labels, pred_labels = align_pred2entity(\n                        prediction[i][:seq_len].cpu().numpy(),\n                        labels[i][:seq_len].cpu().numpy(),\n                        w_ids,\n                        idx2label\n                    )\n            true_list.append(true_labels)\n            pred_list.append(pred_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:49:16.045686Z","iopub.execute_input":"2025-12-17T15:49:16.045976Z","iopub.status.idle":"2025-12-17T15:49:39.778470Z","shell.execute_reply.started":"2025-12-17T15:49:16.045959Z","shell.execute_reply":"2025-12-17T15:49:39.777731Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"print(f1_score(true_list, pred_list,average='macro'))\nprint(classification_report(true_list,  pred_list))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:49:39.779392Z","iopub.execute_input":"2025-12-17T15:49:39.779693Z","iopub.status.idle":"2025-12-17T15:49:40.323721Z","shell.execute_reply.started":"2025-12-17T15:49:39.779661Z","shell.execute_reply":"2025-12-17T15:49:40.322775Z"}},"outputs":[{"name":"stdout","text":"0.8815379226995269\n              precision    recall  f1-score   support\n\n         LOC       0.91      0.94      0.92      1413\n        MISC       0.82      0.80      0.81       708\n         ORG       0.84      0.85      0.84      1220\n         PER       0.95      0.95      0.95      1390\n\n   micro avg       0.89      0.90      0.89      4731\n   macro avg       0.88      0.88      0.88      4731\nweighted avg       0.89      0.90      0.89      4731\n\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"#保存模型\ntorch.save(model.state_dict(),\"bert_ner_baseline_sd.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T15:49:40.324811Z","iopub.execute_input":"2025-12-17T15:49:40.325160Z","iopub.status.idle":"2025-12-17T15:49:41.062147Z","shell.execute_reply.started":"2025-12-17T15:49:40.325127Z","shell.execute_reply":"2025-12-17T15:49:41.061439Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# 模型读取与推理\ndef align_output(output, word_ids, idx2label):\n    output_list =[]\n    previous_word_idx = None\n\n    for i,(out, word_id, idx) in enumerate(zip(output, word_ids, idx2label)):\n        if word_id is None:\n            continue\n        if word_id != previous_word_idx:\n            pred_label = idx2label[out]\n            output_list.append(pred_label)\n        previous_word_idx = word_id\n    return output_list\n\nnum_labels = len(label2idx)\nmodel_infer = Bert(num_labels)\nmodel_infer.load_state_dict(torch.load(\"/kaggle/working/bert_ner_baseline_sd.pth\"))\n\ntext = \"Jughead is in Guangzhou\"\ntokenizer_infer =  BertTokenizerFast.from_pretrained('/kaggle/input/bert-base-cased')\nencoding_infer = tokenizer_infer(\n    text,\n    is_split_into_words = False,\n    return_tensors = 'pt')\n\noutput = model_infer(encoding_infer['input_ids'], encoding_infer['attention_mask'])\noutput = torch.argmax(output, dim=2)\nprint(align_output(output.view(-1).numpy(),encoding_infer.word_ids(),idx2label))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-17T17:29:42.919035Z","iopub.execute_input":"2025-12-17T17:29:42.920081Z","iopub.status.idle":"2025-12-17T17:29:44.047607Z","shell.execute_reply.started":"2025-12-17T17:29:42.920039Z","shell.execute_reply":"2025-12-17T17:29:44.046737Z"}},"outputs":[{"name":"stdout","text":"['B-PER', 'O', 'O', 'B-LOC']\n","output_type":"stream"}],"execution_count":69}]}